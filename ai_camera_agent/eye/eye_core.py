# ai_camera_agent/eye/eye_core.py
"""
Eye æ ¸å¿ƒæ¨¡å— - å…¨ YOLO-World çº§è”æ„ŸçŸ¥æ¶æ„ (æ‰§è¡Œå±‚)

æ ¸å¿ƒå®šä½: å¯ç¼–ç¨‹ã€é«˜ä¿çœŸçš„æ„ŸçŸ¥æ‰§è¡Œå™¨
é€‚é…: Step 4 æ•°æ®åº“é‡æ„ (AsyncDBManager + PerceptionMemory V2)
"""
import asyncio
import logging
from typing import Optional, List, Set, Dict, Any
import numpy as np

from common.types import PerceptionResult, DetectionResult, AnalysisResult
from eye.capture.video_capture import VideoCapture
from eye.capture.frame_buffer import FrameBuffer
from eye.detection.object_detector import ObjectDetector
from eye.filter.state_filter import StateFilter
from eye.analysis.scene_analyzer import SceneAnalyzer
from eye.memory.perception_memory import PerceptionMemory


class EyeCore:
    """
    çœ¼ç›æ ¸å¿ƒç±» - ç»Ÿä¸€ç®¡ç†æ„ŸçŸ¥æµæ°´çº¿
    """

    def __init__(self):
        self.video_capture = VideoCapture()
        self.frame_buffer = FrameBuffer()
        self.object_detector = ObjectDetector()
        self.state_filter = StateFilter()
        self.scene_analyzer = SceneAnalyzer()
        self.perception_memory = PerceptionMemory()

        # è§†é¢‘å½•åˆ¶å™¨
        from eye.capture.video_recorder import VideoRecorder
        self.video_recorder = VideoRecorder()
        self.recording_active = False

        self._running = False
        self._perception_task: Optional[asyncio.Task] = None
        self._recording_task: Optional[asyncio.Task] = None

        # çŠ¶æ€
        self.latest_frame: Optional[np.ndarray] = None
        self.latest_timestamp: float = 0.0
        self.current_event_id: Optional[int] = None
        self.security_policy: str = "æ ‡å‡†æ¨¡å¼"
        self.muted_classes: Set[str] = set()

        logging.info("ğŸ‘ï¸ [Eye] V2 å…¨ YOLO çº§è”æ¶æ„åˆå§‹åŒ–å®Œæˆ")

    async def initialize(self):
        """é‡åˆå§‹åŒ–ï¼ˆå¼‚æ­¥æ“ä½œï¼‰"""
        try:
            # [Step 4] è¿æ¥æ–°ç‰ˆæ•°æ®åº“ç®¡ç†å™¨ (AsyncPG)
            await self.perception_memory.connect_database()
            if self.perception_memory.db_manager:
                if not await self.perception_memory.db_manager.health_check():
                    raise RuntimeError("æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥")
            logging.info("ğŸ‘ï¸ [Eye] åˆå§‹åŒ–å®Œæˆ (æ•°æ®åº“å·²è¿æ¥)")
        except Exception as e:
            logging.error(f"âŒ [Eye] åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def start(self):
        """å¯åŠ¨æ„ŸçŸ¥å¾ªç¯"""
        self._running = True
        logging.info("ğŸ‘ï¸ [Eye] å¯åŠ¨æ„ŸçŸ¥å¾ªç¯...")
        capture_task = asyncio.create_task(self._capture_loop())
        analysis_task = asyncio.create_task(self._analysis_loop())
        recording_task = asyncio.create_task(self._recording_loop())
        await asyncio.gather(capture_task, analysis_task, recording_task)

    async def stop(self):
        """åœæ­¢æ„ŸçŸ¥å¾ªç¯"""
        self._running = False
        await self.video_capture.stop()
        await self.scene_analyzer.close()
        logging.info("ğŸ‘ï¸ [Eye] æ„ŸçŸ¥å¾ªç¯å·²åœæ­¢")

    async def close(self):
        """å…³é—­Eyeæ ¸å¿ƒèµ„æº"""
        if self.perception_memory and self.perception_memory.db_manager:
            await self.perception_memory.db_manager.close_all()
        await self.video_capture.stop()
        await self.scene_analyzer.close()
        logging.info("ğŸ‘ï¸ [Eye] èµ„æºå·²å…³é—­")

    async def _capture_loop(self):
        capture_task = asyncio.create_task(self.video_capture.start())
        try:
            while self._running:
                frame_data = await self.video_capture.get_frame()
                if frame_data:
                    self.latest_frame = frame_data["frame"]
                    self.latest_timestamp = frame_data["timestamp"]
                    await self.frame_buffer.add(frame_data)
                    try:
                        from api.websockets.video_feed import manager
                        await manager.broadcast_frame(self.latest_frame)
                    except Exception as e:
                        logging.debug(f"ğŸ“º WebSocketå¹¿æ’­å¤±è´¥: {e}")
                await asyncio.sleep(0.01)
        finally:
            await self.video_capture.stop()
            capture_task.cancel()

    async def _analysis_loop(self):
        """åˆ†æå¾ªç¯ - æ ¸å¿ƒæµæ°´çº¿"""
        while self._running:
            await self.frame_buffer.wait_for_new_data()
            frames = await self.frame_buffer.get_frames()
            if not frames: continue
            try:
                result = await self.perceive(frames)
                if result:
                    # [Step 4] å­˜å‚¨ç»“æœ (å°†è‡ªåŠ¨è§¦å‘å»é‡å’Œ DB å†™å…¥)
                    await self.perception_memory.store(result)
            except Exception as e:
                logging.error(f"ğŸ‘ï¸ [Eye] åˆ†æé”™è¯¯: {e}")
            await asyncio.sleep(0.01)

    async def _recording_loop(self):
        """å½•åˆ¶å¾ªç¯"""
        while self._running:
            # ç®€åŒ–å½•åˆ¶é€»è¾‘ï¼Œé¿å…è¿‡äºå¤æ‚
            if (self.perception_memory.current_event.is_active and
                    self.perception_memory.current_event.event_id is not None):
                if not self.recording_active:
                    event_id = self.perception_memory.current_event.event_id
                    context_frames = await self.get_context_frames()
                    video_path = self.video_recorder.start_recording(event_id=event_id, frames=context_frames)
                    if video_path:
                        self.recording_active = True
                if self.recording_active and self.latest_frame is not None:
                    self.video_recorder.add_frame(self.latest_frame)
            elif self.recording_active:
                video_path = self.video_recorder.stop_recording()
                if video_path and self.perception_memory.db_manager:
                    try:
                        await self.perception_memory.db_manager.update_video_path(
                            event_id=self.perception_memory.current_event.event_id,
                            video_path=video_path
                        )
                    except Exception as e:
                        logging.error(f"âŒ æ›´æ–°è§†é¢‘è·¯å¾„å¤±è´¥: {e}")
                self.recording_active = False
            await asyncio.sleep(0.1)

    async def perceive(self, frames: List[dict]) -> Optional[PerceptionResult]:
        """æ‰§è¡Œå®Œæ•´æ„ŸçŸ¥æµç¨‹"""
        if not frames: return None
        latest_frame = frames[-1]["frame"]
        timestamp = frames[-1].get("timestamp", "")

        # Step 1: Stage 1 Detect
        detection_result = await self.object_detector.detect_stage1(
            latest_frame,
            alert_targets=self.state_filter.high_priority_classes if hasattr(self.state_filter,
                                                                             'high_priority_classes') else None
        )

        if self.muted_classes:
            detection_result = self._filter_muted(detection_result)

        # Step 2: State Filter
        refine_tasks, vlm_candidates = self.state_filter.check_refinement_needs(
            detection_result.detections
        )

        # Step 3: Stage 2 Refine (æå–ç‰¹å¾)
        refine_features = []
        if refine_tasks:
            refine_features = await self.object_detector.detect_stage2(
                latest_frame,
                refine_tasks
            )

        # Step 4: Assembly
        # [æ ¸å¿ƒ] æŒ‚è½½ç‰¹å¾æ•°æ®ï¼Œä¾› PerceptionMemory ä½¿ç”¨
        alert_tags = set()
        visual_risks = self._check_visual_risks(detection_result)
        if visual_risks: alert_tags.add("visual")

        for f in refine_features:
            if f.get('refine_label') in ['knife', 'gun', 'weapon', 'fire']:
                alert_tags.add(f['refine_label'])

        result = PerceptionResult(
            detection_result=detection_result,
            timestamp=timestamp,
            alert_tags=alert_tags,
            event_id=None  # å°†ç”± Memory å¡«å……
        )

        # [Step 4] å…³é”®: å°† Stage 2 ç‰¹å¾æŒ‚è½½åˆ°ç»“æœå¯¹è±¡
        setattr(result, 'refine_features', refine_features)

        # Step 5: VLM (Optional)
        should_analyze_vlm = (len(vlm_candidates) > 0)
        if should_analyze_vlm and detection_result.detections:
            analysis_result = await self._run_vlm_analysis(
                frames, detection_result.unique_classes
            )
            if analysis_result:
                result.analysis_result = analysis_result
                if analysis_result.is_abnormal:
                    result.alert_tags.add("behavior")

        return result

    async def perceive_single(self, frame: np.ndarray) -> Optional[PerceptionResult]:
        detection_result = await self.object_detector.detect_stage1(frame)
        return PerceptionResult(detection_result=detection_result, timestamp="")

    def _filter_muted(self, detection_result: DetectionResult) -> DetectionResult:
        filtered = [d for d in detection_result.detections if d.class_name not in self.muted_classes]
        return DetectionResult(
            detections=filtered,
            frame=detection_result.frame,
            plotted_frame=detection_result.plotted_frame,
            timestamp=detection_result.timestamp
        )

    def _check_visual_risks(self, detection_result: DetectionResult) -> List[str]:
        current_high_priority = self.state_filter.high_priority_classes
        risks = []
        for det in detection_result.detections:
            if det.class_name in current_high_priority:
                risks.append(det.class_name)
        return risks

    async def _run_vlm_analysis(self, frames: List[dict], detection_labels: List[str]) -> Optional[AnalysisResult]:
        frame_list = [f["frame"] for f in frames]
        return await self.scene_analyzer.analyze(
            frames=frame_list,
            detections=detection_labels,
            security_policy=self.security_policy
        )

    # Command Interfaces
    def update_targets(self, targets: List[str]) -> bool:
        return self.object_detector.update_stage1_targets(targets)

    def update_stage1_targets(self, targets: List[str]) -> bool:
        return self.object_detector.update_stage1_targets(targets)

    def update_stage2_targets(self, targets: List[str]) -> bool:
        return self.object_detector.update_stage2_targets(targets)

    def update_security_policy(self, policy: str, risk_level: str = "normal", dynamic_targets: List[str] = None):
        self.security_policy = policy
        self.state_filter.update_policy(risk_level, dynamic_targets)
        logging.info(f"ğŸ‘ï¸ [Eye] ç­–ç•¥æ›´æ–°: {policy}")

    def mute_class(self, class_name: str):
        self.muted_classes.add(class_name)

    def unmute_class(self, class_name: str):
        self.muted_classes.discard(class_name)

    def get_latest_frame(self) -> Optional[np.ndarray]:
        return self.latest_frame

    async def get_context_frames(self) -> List[np.ndarray]:
        frames = await self.frame_buffer.get_frames()
        return [f["frame"] for f in frames]

    def get_status(self) -> dict:
        return {
            "running": self._running,
            "policy": self.security_policy,
            "filter_status": self.state_filter.get_status(),
            "muted_classes": list(self.muted_classes),
            "current_event_id": self.perception_memory.current_event.event_id
        }